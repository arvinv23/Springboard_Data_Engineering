{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc5af8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting py4j==0.10.9.7 (from pyspark)\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812366 sha256=82faf4f43ef13c44e548fcbd64406c9eca9c93b173b4bcf97713a349f1ca0176\n",
      "  Stored in directory: /Users/aravindh/Library/Caches/pip/wheels/9d/29/ee/3a756632ca3f0a6870933bac1c9db6e4af2c068f019aba0ee1\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.5.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5049fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, avg, sum, min, max\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11bfece8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/path/to/spark/python\")\n",
    "sys.path.append(\"/path/to/spark/python/lib/py4j-<version>-src.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c297627",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/09/17 07:17:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BankingAnalysis\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6ba0439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema for the dataset\n",
    "schema = StructType([\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"surname\", StringType(), True),\n",
    "    StructField(\"credit_score\", IntegerType(), True),\n",
    "    StructField(\"geography\", StringType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"tenure\", IntegerType(), True),\n",
    "    StructField(\"balance\", DoubleType(), True),\n",
    "    StructField(\"num_of_products\", IntegerType(), True),\n",
    "    StructField(\"has_credit_card\", IntegerType(), True),\n",
    "    StructField(\"estimated_salary\", DoubleType(), True),\n",
    "    StructField(\"exited\", IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b6c1af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the CSV file into a DataFrame\n",
    "df = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .load(\"/Users/aravindh/Desktop/Data Engineering/Azure Mini Project/springboard-pyspark-project/pyspark-project/credit card.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fe7c439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a BankingAnalysis class\n",
    "class BankingAnalysis:\n",
    "    def __init__(self, dataframe):\n",
    "        self.df = dataframe\n",
    "    \n",
    "    def total_customers(self):\n",
    "        return self.df.count()\n",
    "    \n",
    "    def customers_by_geography(self):\n",
    "        return self.df.groupBy(\"geography\").agg(count(\"*\").alias(\"num_customers\"))\n",
    "    \n",
    "    def customers_by_gender(self):\n",
    "        return self.df.groupBy(\"gender\").agg(count(\"*\").alias(\"num_customers\"))\n",
    "    \n",
    "    def avg_age_by_geography(self):\n",
    "        return self.df.groupBy(\"geography\").agg(avg(\"age\").alias(\"avg_age\"))\n",
    "    \n",
    "    def avg_balance_by_geography(self):\n",
    "        return self.df.groupBy(\"geography\").agg(avg(\"balance\").alias(\"avg_balance\"))\n",
    "    \n",
    "    def min_max_tenure(self):\n",
    "        return self.df.agg(min(\"tenure\").alias(\"min_tenure\"), max(\"tenure\").alias(\"max_tenure\"))\n",
    "    \n",
    "    def num_credit_card_holders(self):\n",
    "        return self.df.filter(col(\"has_credit_card\") == 1).count()\n",
    "    \n",
    "    def avg_salary_by_gender(self):\n",
    "        return self.df.groupBy(\"gender\").agg(avg(\"estimated_salary\").alias(\"avg_salary\"))\n",
    "    \n",
    "    def num_exited_customers(self):\n",
    "        return self.df.filter(col(\"exited\") == 1).count()\n",
    "    \n",
    "    def total_balance(self):\n",
    "        return self.df.agg(sum(\"balance\").alias(\"total_balance\")).collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbf00a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the BankingAnalysis class\n",
    "analysis = BankingAnalysis(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78ad9f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of customers: 10000\n",
      "Number of customers by geography:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/17 07:17:08 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: CreditScore\n",
      " Schema: geography\n",
      "Expected: geography but found: CreditScore\n",
      "CSV file: file:///Users/aravindh/Desktop/Data%20Engineering/Azure%20Mini%20Project/springboard-pyspark-project/pyspark-project/credit%20card.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+\n",
      "|geography|num_customers|\n",
      "+---------+-------------+\n",
      "|      829|            8|\n",
      "|      675|           37|\n",
      "|      691|           34|\n",
      "|      467|            4|\n",
      "|      800|           10|\n",
      "|      451|            5|\n",
      "|      666|           38|\n",
      "|      591|           31|\n",
      "|      447|            4|\n",
      "|      574|           21|\n",
      "|      475|            6|\n",
      "|      718|           38|\n",
      "|      613|           42|\n",
      "|      577|           34|\n",
      "|      581|           38|\n",
      "|      544|           25|\n",
      "|      747|           22|\n",
      "|      740|           19|\n",
      "|      647|           31|\n",
      "|      711|           39|\n",
      "+---------+-------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Number of customers by gender:\n",
      "+-------+-------------+\n",
      "| gender|num_customers|\n",
      "+-------+-------------+\n",
      "|Germany|         2509|\n",
      "| France|         5014|\n",
      "|  Spain|         2477|\n",
      "+-------+-------------+\n",
      "\n",
      "Average age by geography:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/17 07:17:09 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Geography\n",
      " Schema: gender\n",
      "Expected: gender but found: Geography\n",
      "CSV file: file:///Users/aravindh/Desktop/Data%20Engineering/Azure%20Mini%20Project/springboard-pyspark-project/pyspark-project/credit%20card.csv\n",
      "24/09/17 07:17:09 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: CreditScore, Gender\n",
      " Schema: geography, age\n",
      "Expected: geography but found: CreditScore\n",
      "CSV file: file:///Users/aravindh/Desktop/Data%20Engineering/Azure%20Mini%20Project/springboard-pyspark-project/pyspark-project/credit%20card.csv\n",
      "Java HotSpot(TM) 64-Bit Server VM warning: CodeCache is full. Compiler has been disabled.\n",
      "Java HotSpot(TM) 64-Bit Server VM warning: Try increasing the code cache size using -XX:ReservedCodeCacheSize=\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CodeCache: size=131072Kb used=22384Kb max_used=22384Kb free=108687Kb\n",
      " bounds [0x00000001068c4000, 0x0000000107ec4000, 0x000000010e8c4000]\n",
      " total_blobs=8898 nmethods=7961 adapters=850\n",
      " compilation: disabled (not enough contiguous free space left)\n",
      "+---------+-------+\n",
      "|geography|avg_age|\n",
      "+---------+-------+\n",
      "|      829|   NULL|\n",
      "|      675|   NULL|\n",
      "|      691|   NULL|\n",
      "|      467|   NULL|\n",
      "|      800|   NULL|\n",
      "|      451|   NULL|\n",
      "|      666|   NULL|\n",
      "|      591|   NULL|\n",
      "|      447|   NULL|\n",
      "|      574|   NULL|\n",
      "|      475|   NULL|\n",
      "|      718|   NULL|\n",
      "|      613|   NULL|\n",
      "|      577|   NULL|\n",
      "|      581|   NULL|\n",
      "|      544|   NULL|\n",
      "|      747|   NULL|\n",
      "|      740|   NULL|\n",
      "|      647|   NULL|\n",
      "|      711|   NULL|\n",
      "+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Average balance by geography:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/17 07:17:10 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: CreditScore, Tenure\n",
      " Schema: geography, balance\n",
      "Expected: geography but found: CreditScore\n",
      "CSV file: file:///Users/aravindh/Desktop/Data%20Engineering/Azure%20Mini%20Project/springboard-pyspark-project/pyspark-project/credit%20card.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+\n",
      "|geography|       avg_balance|\n",
      "+---------+------------------+\n",
      "|      829|              6.25|\n",
      "|      675| 5.216216216216216|\n",
      "|      691| 5.617647058823529|\n",
      "|      467|               6.5|\n",
      "|      800|               4.5|\n",
      "|      451|               6.6|\n",
      "|      666|4.7105263157894735|\n",
      "|      591| 5.419354838709677|\n",
      "|      447|               5.0|\n",
      "|      574| 4.333333333333333|\n",
      "|      475| 5.333333333333333|\n",
      "|      718|5.2894736842105265|\n",
      "|      613| 5.761904761904762|\n",
      "|      577| 4.823529411764706|\n",
      "|      581| 4.026315789473684|\n",
      "|      544|              5.24|\n",
      "|      747| 5.454545454545454|\n",
      "|      740| 5.473684210526316|\n",
      "|      647| 4.903225806451613|\n",
      "|      711| 5.051282051282051|\n",
      "+---------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Minimum and maximum tenure: [Row(min_tenure=18, max_tenure=92)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/17 07:17:10 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Age\n",
      " Schema: tenure\n",
      "Expected: tenure but found: Age\n",
      "CSV file: file:///Users/aravindh/Desktop/Data%20Engineering/Azure%20Mini%20Project/springboard-pyspark-project/pyspark-project/credit%20card.csv\n",
      "24/09/17 07:17:10 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: NumOfProducts\n",
      " Schema: has_credit_card\n",
      "Expected: has_credit_card but found: NumOfProducts\n",
      "CSV file: file:///Users/aravindh/Desktop/Data%20Engineering/Azure%20Mini%20Project/springboard-pyspark-project/pyspark-project/credit%20card.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of credit card holders: 5084\n",
      "Average salary by gender:\n",
      "+-------+-------------------+\n",
      "| gender|         avg_salary|\n",
      "+-------+-------------------+\n",
      "|Germany|0.49740932642487046|\n",
      "| France| 0.5167530913442362|\n",
      "|  Spain| 0.5296729915220024|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/17 07:17:10 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Geography, IsActiveMember\n",
      " Schema: gender, estimated_salary\n",
      "Expected: gender but found: Geography\n",
      "CSV file: file:///Users/aravindh/Desktop/Data%20Engineering/Azure%20Mini%20Project/springboard-pyspark-project/pyspark-project/credit%20card.csv\n",
      "24/09/17 07:17:10 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: EstimatedSalary\n",
      " Schema: exited\n",
      "Expected: exited but found: EstimatedSalary\n",
      "CSV file: file:///Users/aravindh/Desktop/Data%20Engineering/Azure%20Mini%20Project/springboard-pyspark-project/pyspark-project/credit%20card.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of customers who have exited: 0\n",
      "Total balance across all customers: 50128.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/17 07:17:10 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Tenure\n",
      " Schema: balance\n",
      "Expected: balance but found: Tenure\n",
      "CSV file: file:///Users/aravindh/Desktop/Data%20Engineering/Azure%20Mini%20Project/springboard-pyspark-project/pyspark-project/credit%20card.csv\n"
     ]
    }
   ],
   "source": [
    "# Perform analysis and print the results\n",
    "print(\"Total number of customers:\", analysis.total_customers())\n",
    "print(\"Number of customers by geography:\")\n",
    "analysis.customers_by_geography().show()\n",
    "print(\"Number of customers by gender:\")\n",
    "analysis.customers_by_gender().show()\n",
    "print(\"Average age by geography:\")\n",
    "analysis.avg_age_by_geography().show()\n",
    "print(\"Average balance by geography:\")\n",
    "analysis.avg_balance_by_geography().show()\n",
    "print(\"Minimum and maximum tenure:\", analysis.min_max_tenure().collect())\n",
    "print(\"Number of credit card holders:\", analysis.num_credit_card_holders())\n",
    "print(\"Average salary by gender:\")\n",
    "analysis.avg_salary_by_gender().show()\n",
    "print(\"Number of customers who have exited:\", analysis.num_exited_customers())\n",
    "print(\"Total balance across all customers:\", analysis.total_balance())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59e91d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PrintLoanDataset\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = spark.read.csv(\"/Users/aravindh/Desktop/Data Engineering/Azure Mini Project/springboard-pyspark-project/pyspark-project/loan.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc232a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+------+------------+--------------+-----------+------+-----------+-------------+-------------+-----------+-------+------------+----------------+------------------+\n",
      "|Customer_ID|Age|Gender|  Occupation|Marital Status|Family Size|Income|Expenditure|Use Frequency|Loan Category|Loan Amount|Overdue| Debt Record| Returned Cheque| Dishonour of Bill|\n",
      "+-----------+---+------+------------+--------------+-----------+------+-----------+-------------+-------------+-----------+-------+------------+----------------+------------------+\n",
      "|    IB14001| 30|  MALE|BANK MANAGER|        SINGLE|          4| 50000|      22199|            6|      HOUSING| 10,00,000 |      5|      42,898|               6|                 9|\n",
      "|    IB14008| 44|  MALE|   PROFESSOR|       MARRIED|          6| 51000|      19999|            4|     SHOPPING|     50,000|      3|      33,999|               1|                 5|\n",
      "|    IB14012| 30|FEMALE|     DENTIST|        SINGLE|          3| 58450|      27675|            5|   TRAVELLING|     75,000|      6|      20,876|               3|                 1|\n",
      "|    IB14018| 29|  MALE|     TEACHER|       MARRIED|          5| 45767|      12787|            3|    GOLD LOAN|  6,00,000 |      7|      11,000|               0|                 4|\n",
      "|    IB14022| 34|  MALE|      POLICE|        SINGLE|          4| 43521|      11999|            3|   AUTOMOBILE|  2,00,000 |      2|      43,898|               1|                 2|\n",
      "+-----------+---+------+------------+--------------+-----------+------+-----------+-------------+-------------+-----------+-------+------------+----------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the first 5 lines of the DataFrame\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0bd072f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 500\n"
     ]
    }
   ],
   "source": [
    "# Print the number of rows in the loan dataset\n",
    "print(\"Number of rows:\", df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1371485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct records: 500\n"
     ]
    }
   ],
   "source": [
    "# Print the count of distinct records in the loan dataset\n",
    "print(\"Number of distinct records:\", df.distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a043e0cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n",
      "|     Loan Category|count|\n",
      "+------------------+-----+\n",
      "|           HOUSING|   67|\n",
      "|        TRAVELLING|   53|\n",
      "|       BOOK STORES|    7|\n",
      "|       AGRICULTURE|   12|\n",
      "|         GOLD LOAN|   77|\n",
      "|  EDUCATIONAL LOAN|   20|\n",
      "|        AUTOMOBILE|   60|\n",
      "|          BUSINESS|   24|\n",
      "|COMPUTER SOFTWARES|   35|\n",
      "|           DINNING|   14|\n",
      "|          SHOPPING|   35|\n",
      "|       RESTAURANTS|   41|\n",
      "|       ELECTRONICS|   14|\n",
      "|          BUILDING|    7|\n",
      "|        RESTAURANT|   20|\n",
      "|   HOME APPLIANCES|   14|\n",
      "+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the number of loans in each category\n",
    "df.groupBy(\"Loan Category\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd490759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of people with loan amount greater than 1 lack: 0\n"
     ]
    }
   ],
   "source": [
    "# Find the number of people who have taken more than 1 lack loan\n",
    "count = df.filter(df[\"Loan Amount\"] > 100000).count()\n",
    "print(\"Number of people with loan amount greater than 1 lack:\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78dcfe72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of people with income greater than 60000 rupees: 198\n"
     ]
    }
   ],
   "source": [
    "# Find the number of people with income greater than 60000 rupees\n",
    "count = df.filter(df[\"Income\"] > 60000).count()\n",
    "print(\"Number of people with income greater than 60000 rupees:\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "59ad023b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of people with 2 or more returned cheques and income less than 50000: 137\n"
     ]
    }
   ],
   "source": [
    "count = df.filter((df[\" Returned Cheque\"] >= 2) & (df[\"Income\"] < 50000)).count()\n",
    "print(\"Number of people with 2 or more returned cheques and income less than 50000:\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "abe7ba7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of people with 2 or more returned cheques and are single: 111\n"
     ]
    }
   ],
   "source": [
    "# Find the number of people with 2 or more returned cheques and are single\n",
    "count = df.filter((df[\" Returned Cheque\"] >= 2) & (df[\"Marital Status\"] == \"SINGLE\")).count()\n",
    "print(\"Number of people with 2 or more returned cheques and are single:\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "10abdb49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of people with expenditure over 50000 a month: 6\n"
     ]
    }
   ],
   "source": [
    "# Find the number of people with expenditure over 50000 a month\n",
    "count = df.filter(df[\"Expenditure\"] > 50000).count()\n",
    "print(\"Number of people with expenditure over 50000 a month:\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f66ce6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_card_df = spark.read.csv(\"/Users/aravindh/Desktop/Data Engineering/Azure Mini Project/springboard-pyspark-project/pyspark-project/credit card.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3b4bd83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- RowNumber: integer (nullable = true)\n",
      " |-- CustomerId: integer (nullable = true)\n",
      " |-- Surname: string (nullable = true)\n",
      " |-- CreditScore: integer (nullable = true)\n",
      " |-- Geography: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Tenure: integer (nullable = true)\n",
      " |-- Balance: double (nullable = true)\n",
      " |-- NumOfProducts: integer (nullable = true)\n",
      " |-- IsActiveMember: integer (nullable = true)\n",
      " |-- EstimatedSalary: double (nullable = true)\n",
      " |-- Exited: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "credit_card_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "280ad851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns: 13\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of columns:\", len(credit_card_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "09ca22aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 10000\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of rows:\", credit_card_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e57961a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct records: 10000\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of distinct records:\", credit_card_df.distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1277b6e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----------+---------+------+---+------+---------+-------------+--------------+---------------+------+\n",
      "|RowNumber|CustomerId| Surname|CreditScore|Geography|Gender|Age|Tenure|  Balance|NumOfProducts|IsActiveMember|EstimatedSalary|Exited|\n",
      "+---------+----------+--------+-----------+---------+------+---+------+---------+-------------+--------------+---------------+------+\n",
      "|        1|  15634602|Hargrave|        619|   France|Female| 42|     2|      0.0|            1|             1|      101348.88|     1|\n",
      "|        2|  15647311|    Hill|        608|    Spain|Female| 41|     1| 83807.86|            1|             1|      112542.58|     0|\n",
      "|        3|  15619304|    Onio|        502|   France|Female| 42|     8| 159660.8|            3|             0|      113931.57|     1|\n",
      "|        4|  15701354|    Boni|        699|   France|Female| 39|     1|      0.0|            2|             0|       93826.63|     0|\n",
      "|        5|  15737888|Mitchell|        850|    Spain|Female| 43|     2|125510.82|            1|             1|        79084.1|     0|\n",
      "+---------+----------+--------+-----------+---------+------+---+------+---------+-------------+--------------+---------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "credit_card_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "669aebdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of members eligible for credit card: 8522\n"
     ]
    }
   ],
   "source": [
    "eligible_members = credit_card_df.filter((col(\"Age\") >= 18) & (col(\"EstimatedSalary\") > 30000)).count()\n",
    "print(\"Number of members eligible for credit card:\", eligible_members)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ae94adb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of members eligible and active in the bank: 4379\n"
     ]
    }
   ],
   "source": [
    "eligible_active_members = credit_card_df.filter(\n",
    "    (col(\"Age\") >= 18) & \n",
    "    (col(\"EstimatedSalary\") > 30000) & \n",
    "    (col(\"IsActiveMember\") == 1)\n",
    ").count()\n",
    "print(\"Number of members eligible and active in the bank:\", eligible_active_members)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b88b3102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of credit card users with Estimated Salary greater than 100000 and have exited the card: 1044\n"
     ]
    }
   ],
   "source": [
    "users_salary_gt_100k_exited = credit_card_df.filter((col(\"EstimatedSalary\") > 100000) & (col(\"Exited\") == 1)).count()\n",
    "print(\"Number of credit card users with Estimated Salary greater than 100000 and have exited the card:\", users_salary_gt_100k_exited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "87540fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of credit card users with Estimated Salary less than 100000 and have more than 1 products: 2432\n"
     ]
    }
   ],
   "source": [
    "users_salary_lt_100k_multiple_products = credit_card_df.filter((col(\"EstimatedSalary\") < 100000) & (col(\"NumOfProducts\") > 1)).count()\n",
    "print(\"Number of credit card users with Estimated Salary less than 100000 and have more than 1 products:\", users_salary_lt_100k_multiple_products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "416a06a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the transaction dataset\n",
    "transaction_df = spark.read.csv(\"/Users/aravindh/Desktop/Data Engineering/Azure Mini Project/springboard-pyspark-project/pyspark-project/txn.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ab375e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------------+\n",
      "|   Account No|max_withdrawal_amount|\n",
      "+-------------+---------------------+\n",
      "|409000438611'|                2.4E8|\n",
      "|     1196711'|        4.594475464E8|\n",
      "|     1196428'|                1.5E8|\n",
      "|409000493210'|                1.5E7|\n",
      "|409000611074'|             912000.0|\n",
      "|409000425051'|               3.54E8|\n",
      "|409000405747'|                1.7E8|\n",
      "|409000493201'|            2500000.0|\n",
      "|409000438620'|                4.0E8|\n",
      "|409000362497'|        1.413662392E8|\n",
      "+-------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the maximum withdrawal amount for each account\n",
    "max_withdrawal_df = transaction_df.groupBy(\"Account No\").agg(max(\" WITHDRAWAL AMT \").alias(\"max_withdrawal_amount\"))\n",
    "# Show the result\n",
    "max_withdrawal_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "626a2589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------+\n",
      "|   Account No|max_deposit_amount|\n",
      "+-------------+------------------+\n",
      "|409000438611'|          1.7025E8|\n",
      "|     1196711'|             5.0E8|\n",
      "|     1196428'|     2.119594422E8|\n",
      "|409000493210'|             1.5E7|\n",
      "|409000611074'|         3000000.0|\n",
      "|409000425051'|             1.5E7|\n",
      "|409000405747'|           2.021E8|\n",
      "|409000493201'|         1000000.0|\n",
      "|409000438620'|           5.448E8|\n",
      "|409000362497'|             2.0E8|\n",
      "+-------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the maximum deposit amount of an account\n",
    "max_deposit_df = transaction_df.groupBy(\"Account No\").agg(max(\" DEPOSIT AMT \").alias(\"max_deposit_amount\"))\n",
    "max_deposit_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e9758c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------+\n",
      "|   Account No|min_deposit_amount|\n",
      "+-------------+------------------+\n",
      "|409000438611'|              0.03|\n",
      "|     1196711'|              1.01|\n",
      "|     1196428'|               1.0|\n",
      "|409000493210'|              0.01|\n",
      "|409000611074'|            1320.0|\n",
      "|409000425051'|               1.0|\n",
      "|409000405747'|             500.0|\n",
      "|409000493201'|               0.9|\n",
      "|409000438620'|              0.07|\n",
      "|409000362497'|              0.03|\n",
      "+-------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the minimum deposit amount of an account\n",
    "min_deposit_df = transaction_df.groupBy(\"Account No\").agg(min(\" DEPOSIT AMT \").alias(\"min_deposit_amount\"))\n",
    "min_deposit_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b432562d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+\n",
      "|   Account No|       total_balance|\n",
      "+-------------+--------------------+\n",
      "|409000438611'|-2.49486577068339...|\n",
      "|     1196711'|-1.60476498101275E13|\n",
      "|     1196428'| -8.1418498130721E13|\n",
      "|409000493210'|-3.27584952132095...|\n",
      "|409000611074'|       1.615533622E9|\n",
      "|409000425051'|-3.77211841164998...|\n",
      "|409000405747'|-2.43108047067000...|\n",
      "|409000493201'|1.0420831829499985E9|\n",
      "|409000438620'|-7.12291867951358...|\n",
      "|409000362497'| -5.2860004792808E13|\n",
      "+-------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate the sum of balance in every bank account\n",
    "balance_sum_df = transaction_df.groupBy(\"Account No\").agg(sum(\"BALANCE AMT\").alias(\"total_balance\"))\n",
    "balance_sum_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2005a388",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 96:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------------+\n",
      "|transaction_date|transaction_count|\n",
      "+----------------+-----------------+\n",
      "|            NULL|           116201|\n",
      "+----------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, to_date, count\n",
    "\n",
    "# Find the number of transactions on each date\n",
    "transaction_count_df = transaction_df.withColumn(\"transaction_date\", to_date(\"VALUE DATE\", \"dd-MM-yyyy\")) \\\n",
    "                                      .groupBy(\"transaction_date\") \\\n",
    "                                      .agg(count(\"*\").alias(\"transaction_count\"))\n",
    "transaction_count_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fa453246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------------+\n",
      "|   Account No| WITHDRAWAL AMT |\n",
      "+-------------+----------------+\n",
      "|409000611074'|        133900.0|\n",
      "|409000611074'|        195800.0|\n",
      "|409000611074'|        143800.0|\n",
      "|409000611074'|        331650.0|\n",
      "|409000611074'|        129000.0|\n",
      "|409000611074'|        230013.0|\n",
      "|409000611074'|        367900.0|\n",
      "|409000611074'|        108000.0|\n",
      "|409000611074'|        141000.0|\n",
      "|409000611074'|        206000.0|\n",
      "|409000611074'|        242300.0|\n",
      "|409000611074'|        113250.0|\n",
      "|409000611074'|        206900.0|\n",
      "|409000611074'|        276000.0|\n",
      "|409000611074'|        171000.0|\n",
      "|409000611074'|        189800.0|\n",
      "|409000611074'|        271323.0|\n",
      "|409000611074'|        200600.0|\n",
      "|409000611074'|        176900.0|\n",
      "|409000611074'|        150050.0|\n",
      "+-------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List of customers with withdrawal amount more than 1 lakh\n",
    "high_withdrawal_customers_df = transaction_df.filter(col(\" WITHDRAWAL AMT \") > 100000) \\\n",
    "                                              .select(\"Account No\", \" WITHDRAWAL AMT \")\n",
    "high_withdrawal_customers_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f085304",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
